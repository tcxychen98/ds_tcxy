# -*- coding: utf-8 -*-
"""X-Fab Technical Assessment Notebook (Timothy Chen Xian Yii)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PLmNqU24E402Pz8IxykDog8nDrk3ctcE

# Introduction

In the semiconductor industry, a wafer goes through fabrication process that involves various tools, and a recipe defines how the wafer is processed by a tool. A wafer lot is a batch of multiple wafers that belong to the same technology and product.The sensor values of a tool are recorded for each wafer run, and this data is important for engineers to determine if the tool is working properly or not. Therefore, an FDC (fault detection and classification) system is necessary to detect any tool sensor anomalies to prevent further wafer scraps.

## Importing Dataset & Libraries
"""

# Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")
pd.set_option('display.max_columns', None)

# Load the dataset
data = pd.read_csv('/content/drive/MyDrive/X-Fab Sarawak/Tool_Sensor_Data.csv')

"""# Data Pre-Processing

"""

# Display the five rows of the dataset
print("First five rows of the dataset:")
data.head()

# Checking for missing & duplicate values
pd.set_option('display.max_rows', None)
print("\nChecking for missing values:")
missing_values = data.isnull().sum()

print(missing_values)

# Remove missing values
clean_df_ver1 = data.dropna(axis=1, thresh=len(data)-10000) #remove columns with more than 10k missing data
clean_df_ver1 = clean_df_ver1.dropna() #remove rows with missing values

# Renaming sensor data
for i in range(22, 68):
    clean_df_ver1.rename(columns={clean_df_ver1.columns[i]: f'Sensor {i-21}'}, inplace=True)

print("\nOverview of cleaned data")
clean_df_ver1.info()

# Display unique attributes
unique_values = {}
for column in clean_df_ver1.columns:
    unique_values[column] = clean_df_ver1[column].unique()

for column, values in unique_values.items():
    print(f"Unique values:'{column}':")
    print(values)
    print()

# Remove attributes with only one unique numerical value (irrelevant)
def remove_singular(df,threshold):
  df_copy = df.copy()
  features = df_copy.select_dtypes(include=['int', 'float']).columns
  for column in features:
        unique_count = df[column].nunique()
        if unique_count <= threshold:
            df.drop(column, axis=1, inplace=True)
  return df_copy

clean_df_ver2 = remove_singular(clean_df_ver1, 2)

# Outlier detection using z-score methods
def outlier_iqr(df, threshold):
  df_copy = df.copy()
  features = df_copy.select_dtypes(include=['int', 'float']).columns
  for column in features:
      z_scores = (df_copy[column] - df_copy[column].mean()) / df_copy[column].std()
      outliers = df_copy[abs(z_scores) > threshold]
      df_copy = df_copy.drop(outliers.index, axis=0)
  return df_copy

clean_df_ver3 = outlier_iqr(clean_df_ver2, 3.0)
clean_df_ver3.describe()

"""# Exploratory Data Analysis

"""

# Histogram plot (may be time consuming)
numerical_columns = clean_df_ver3.select_dtypes(include=['int', 'float']).columns
plt.figure(figsize=(12,8))
for i, feature in enumerate(numerical_columns):
    plt.subplot(7,8,i+1)
    sns.histplot(clean_df_ver3[feature], kde=True)
    plt.title('')  # Remove title
    plt.xlabel('')  # Remove x-axis label
    plt.ylabel('')  # Remove y-axis label
plt.tight_layout()
plt.show()

# Create interactive function to plot selected column
from ipywidgets import interact, Dropdown
numerical_columns = clean_df_ver3.select_dtypes(include=['int', 'float']).columns

@interact(column=Dropdown(options=numerical_columns, description='Select Column'))
def plot_histogram(column):
    plt.figure(figsize=(6, 4))
    sns.histplot(clean_df_ver3[column], kde=True)
    plt.title(f'Distribution of {column}', fontsize=8)
    plt.xlabel(column, fontsize=8)
    plt.ylabel('Frequency', fontsize=8)
    plt.show()

# Boxplots (diagram may be congested)
numerical_columns = clean_df_ver3.select_dtypes(include=['int', 'float']).columns
plt.figure(figsize=(12,8))
for i, feature in enumerate(numerical_columns):
    plt.subplot(7,8,i+1)
    sns.boxplot(x=clean_df_ver3[feature])
    plt.title('')  # Remove title
    plt.xlabel('')  # Remove x-axis label
    plt.ylabel('')  # Remove y-axis label
plt.tight_layout()
plt.show()

# Create interactive function to plot selected column
from ipywidgets import interact, Dropdown
numerical_columns = clean_df_ver3.select_dtypes(include=['int', 'float']).columns

@interact(column=Dropdown(options=numerical_columns, description='Select Column'))
def plot_boxplots(column):
    plt.figure(figsize=(6, 4))
    sns.boxplot(clean_df_ver3[column])
    plt.title(f'Distribution of {column}', fontsize=8)
    plt.xlabel(column, fontsize=8)
    plt.ylabel('Frequency', fontsize=8)
    plt.show()

# Pie Chart
from ipywidgets import interact, Dropdown
columns_of_interest = ['ProductGrpID', 'RouteID', 'Technology']

@interact(column=Dropdown(options=columns_of_interest, description='Select Column'))
def plot_piechart(column):
  column_counts = clean_df_ver3[column].value_counts()
  plt.figure(figsize=(8,6))
  plt.pie(column_counts, labels=column_counts.index, autopct='%1.1f%%', textprops={'fontsize': 8})
  plt.title(f'Distribution of {column}')
  plt.axis('equal')
  plt.show()

# Interactive Lineplot Dashboard
!pip install dash
import dash
from dash import dcc, html
from dash.dependencies import Input, Output
import plotly.graph_objs as go

# Get unique ProductID values
product_ids = clean_df_ver3['ProductID'].unique()

# Get the numerical sensor columns from the DataFrame
numerical_columns = [col for col in clean_df_ver3.columns if col.startswith('Sensor')]

# Initialize the Dash app
app = dash.Dash(__name__)

# Define the layout
app.layout = html.Div([
    html.Label('Select ProductID'),
    dcc.Dropdown(
        id='dropdown-product-id',
        options=[{'label': str(product_id), 'value': product_id} for product_id in product_ids],
        value=product_ids[0]  # Default to the first ProductID
    ),
    html.Label('Select Sensor'),
    dcc.Dropdown(
        id='dropdown-sensor',
        options=[{'label': col, 'value': col} for col in numerical_columns],
        value=numerical_columns[0]  # Default to the first sensor
    ),
    dcc.Graph(id='line-plot')
])

# Define callback to update the line plot
@app.callback(
    Output('line-plot', 'figure'),
    [Input('dropdown-product-id', 'value'),
     Input('dropdown-sensor', 'value')]
)
def update_line_plot(product_id, sensor):
    plt.figure(figsize=(10, 6))

    # Filter the DataFrame for the selected product ID and sensor
    filtered_df = clean_df_ver3[(clean_df_ver3['ProductID'] == product_id) & (clean_df_ver3[sensor].notna())]

    # Create a line for each run
    data = []
    for run in filtered_df['Run'].unique():
        run_data = filtered_df[filtered_df['Run'] == run]
        line = go.Scatter(
            x=run_data.index,
            y=run_data[sensor],
            mode='lines',
            name=f'Run {run}'
        )
        data.append(line)

    # Create the plot layout
    layout = go.Layout(
        title=f'Sensory Data of {sensor} for ProductID {product_id}',
        xaxis={'title': 'Index'},
        yaxis={'title': sensor},
        showlegend=True
    )
    return {'data': data, 'layout': layout}

# Run the app
if __name__ == '__main__':
    app.run_server(debug=True)

"""# Modelling"""

# Categorical Encoding
from sklearn.preprocessing import LabelEncoder

label_encoders = {}
clean_df_ver4 = clean_df_ver3.copy()
for column in clean_df_ver4.select_dtypes(include=['object']).columns:
    label_encoders[column] = LabelEncoder()
    clean_df_ver4[column + '_encoded'] = label_encoders[column].fit_transform(clean_df_ver4[column])
    clean_df_ver4.drop(columns=[column], inplace=True)  # Drop the original column

clean_df_ver4.describe()

# Create labels with K Means
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Normalize the features
scaler = StandardScaler()
clean_df_ver5 = scaler.fit_transform(clean_df_ver4)

# K-means clustering
kmeans = KMeans(n_clusters=2, random_state=42)
kmeans_labels = kmeans.fit_predict(clean_df_ver5)

# Add cluster labels, 0s and 1s, as new columns
clean_df_ver6 = clean_df_ver4.copy()
clean_df_ver6['KMeansLabel'] = kmeans_labels
clean_df_ver6['KMeansLabel'] = clean_df_ver6['KMeansLabel'].map({0: 0, 1: 1})

# Display the distribution of hypothetical labels
clean_df_ver6['KMeansLabel'].value_counts()

# Fit a random forest classifier and get feature importances
from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(random_state=8181)
rf.fit(clean_df_ver6.drop(columns=['KMeansLabel']), clean_df_ver6['KMeansLabel'])
importances = rf.feature_importances_

# Visualize importance
plt.figure(figsize=(20,14))
plt.barh(clean_df_ver6.drop(columns=['KMeansLabel']).columns, importances)
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.title('Random Forest Feature Importances')
plt.show()

# Modify data to only inlcude important variables
importance_table = pd.Series(importances, index=clean_df_ver6.drop(columns=['KMeansLabel']).columns)
selected_features = importance_table[importance_table > 0.02]
selected_feature_names = selected_features.index.tolist()
selected_feature_names.append('KMeansLabel')
clean_df_ver7 = clean_df_ver6[selected_feature_names]

# Train Test Splitting
from sklearn.model_selection import train_test_split

X = clean_df_ver7.drop(['KMeansLabel'], axis=1)
y = clean_df_ver7['KMeansLabel']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=8181)
X_train.shape, X_test.shape, y_train.shape, y_test.shape

from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import GaussianNB

# Decision Tree (DT)
model_1 = DecisionTreeClassifier(random_state=8181)
model_1.fit(X_train, y_train)
y_pred_1 = model_1.predict(X_test)

# Support Vector Machine (SVM)
model_2 = SVC(C=100.0, probability=True, random_state=8181)
model_2.fit(X_train, y_train)
y_pred_2 = model_2.predict(X_test)

# Naive Bayes (NB)
model_3 = GaussianNB()
model_3.fit(X_train, y_train)
y_pred_3 = model_3.predict(X_test)

from tabulate import tabulate
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix

model_names = ['DT', 'SVM', 'NB']

# Create list to store metric scores
accuracy_scores = []
precision_scores = []
recall_scores = []
f1_scores = []

# Compute metrics for each model
for y_pred in [y_pred_1, y_pred_2, y_pred_3]:
    accuracy = accuracy_score(y_test, y_pred) * 100
    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')

    accuracy_scores.append(accuracy)
    precision_scores.append(precision)
    recall_scores.append(recall)
    f1_scores.append(f1)

# Print the table
table_data = list(zip(model_names, accuracy_scores, precision_scores, recall_scores, f1_scores))
table_headers = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score']
table = tabulate(table_data, headers=table_headers, tablefmt='grid')
print(table)

# Cross Validation
from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import make_scorer

# Set up k-fold cross-validation
model_list = [model_1,model_2,model_3]
k_fold = KFold(n_splits=5, shuffle=True, random_state=8181)

# Perform cross-validation for each model
i = 0
for model in model_list:
    accuracy_scores = cross_val_score(model, X, y, cv=k_fold, scoring=make_scorer(accuracy_score))

    # Print the results for each model
    print(f"{model_names[i]} - Cross-validated Accuracy Scores:", accuracy_scores)
    print(f"{model_names[i]} - Mean Accuracy:", round(np.mean(accuracy_scores),3))
    print()
    i += 1

from imblearn.under_sampling import RandomUnderSampler

# Create an instance of RandomUnderSampler
undersampler = RandomUnderSampler(random_state=8181)

# Apply undersampling to your dataset
X_resampled, y_resampled = undersampler.fit_resample(X, y)

# Decision Tree (DT)
model_1_res = DecisionTreeClassifier(random_state=8181)
model_1_res.fit(X_resampled, y_resampled)
y_pred_1_res = model_1_res.predict(X_test)

# Support Vector Machine (SVM)
model_2_res = SVC(C=100.0, probability=True, random_state=8181)
model_2_res.fit(X_resampled, y_resampled)
y_pred_2_res = model_2_res.predict(X_test)

# Naive Bayes (NB)
model_3_res = GaussianNB()
model_3_res.fit(X_resampled, y_resampled)
y_pred_3_res = model_3_res.predict(X_test)

from tabulate import tabulate
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix

model_names = ['DT', 'SVM', 'NB']

# Create list to store metric scores
accuracy_scores = []
precision_scores = []
recall_scores = []
f1_scores = []

# Compute metrics for each model
for y_pred in [y_pred_1_res, y_pred_2_res, y_pred_3_res]:
    accuracy = accuracy_score(y_test, y_pred) * 100
    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='weighted')

    accuracy_scores.append(accuracy)
    precision_scores.append(precision)
    recall_scores.append(recall)
    f1_scores.append(f1)

# Print the table
table_data = list(zip(model_names, accuracy_scores, precision_scores, recall_scores, f1_scores))
table_headers = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score']
table = tabulate(table_data, headers=table_headers, tablefmt='grid')
print(table)

# Cross Validation (Resampled)
from sklearn.model_selection import cross_val_score, KFold
from sklearn.metrics import make_scorer

# Set up k-fold cross-validation
model_list = [model_1_res,model_2_res,model_3_res]
k_fold = KFold(n_splits=5, shuffle=True, random_state=8181)

# Perform cross-validation for each model
i = 0
for model in model_list:
    accuracy_scores = cross_val_score(model, X, y, cv=k_fold, scoring=make_scorer(accuracy_score))

    # Print the results for each model
    print(f"{model_names[i]} - Cross-validated Accuracy Scores:", accuracy_scores)
    print(f"{model_names[i]} - Mean Accuracy:", round(np.mean(accuracy_scores),3))
    print()
    i += 1

"""# Deployment

"""

!pip install streamlit

import streamlit as st

# Define the input interface using Streamlit widgets
def input_interface(features):
    st.sidebar.header('Feature Inputs')
    feature_values = []
    for i, feature in enumerate(features):
        value = st.sidebar.number_input(f'{feature}', value=0.5)
        feature_values.append(value)
    return feature_values

# Define the prediction function for each model
def predict_decision_tree(features):
    clf = DecisionTreeClassifier()
    clf.fit([features], [0])  # Placeholder data for demonstration
    prediction = clf.predict([features])
    return prediction

def predict_svm(features):
    clf = SVC()
    clf.fit([features], [0])  # Placeholder data for demonstration
    prediction = clf.predict([features])
    return prediction

def predict_naive_bayes(features):
    clf = GaussianNB()
    clf.fit([features], [0])  # Placeholder data for demonstration
    prediction = clf.predict([features])
    return prediction

# Main function to run the Streamlit app
def main():
    st.title('Defect Prediction')

    # Model selection
    model = st.sidebar.selectbox('Select Model', ['Decision Tree', 'SVM', 'Naive Bayes'])

    # Feature list
    features = clean_df_ver7.drop(['KMeansLabel'], axis=1)  # Add more features as needed

    feature_values = input_interface(features)

    # Button to trigger prediction
    if st.sidebar.button('Predict'):
        if model == 'Decision Tree':
            prediction = predict_decision_tree(feature_values)
        elif model == 'SVM':
            prediction = predict_svm(feature_values)
        elif model == 'Naive Bayes':
            prediction = predict_naive_bayes(feature_values)

        st.write('Prediction:', 'Defect' if prediction == 1 else 'Non-defect')

if __name__ == '__main__':
    main()